{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJIAQTsAzkZ0"
      },
      "source": [
        "# LLM prompting for entity labeling\n",
        "This notebook contains starter code for prompting an LLM API for the task of entity recognition."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install ipytest\n",
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install evaluate\n",
        "!pip install seqeval\n",
        "!pip install ratelimit\n",
        "!pip install cohere"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This code block just contains standard setup code for running in Python\n",
        "import json\n",
        "import string\n",
        "import re\n",
        "import time\n",
        "from tqdm.auto import tqdm\n",
        "import random\n",
        "\n",
        "# PyTorch imports\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, BertModel, DefaultDataCollator\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "import evaluate\n",
        "from ratelimit import limits\n",
        "import cohere"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "UjihAVr90bDo"
      },
      "outputs": [],
      "source": [
        "# Fix the random seed(s) for reproducability\n",
        "random_seed = 8942764\n",
        "torch.random.manual_seed(random_seed)\n",
        "torch.cuda.manual_seed(random_seed)\n",
        "np.random.seed(random_seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Just a helper function for efficiently removing punctuation from a string\n",
        "def strip_punct(s):  return s.translate(string.punctuation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up LLM backend. They are mostly the same, but some slight differences.\n",
        "\n",
        "# Initialize Cohere LLM client with your API key.\n",
        "# You can register for an account here: https://dashboard.cohere.ai/welcome/register\n",
        "# Then you can find your API key here: https://dashboard.cohere.com/api-keys\n",
        "co = cohere.Client('API-KEY')\n",
        "USER_STR = \"USER\"\n",
        "SYSTEM_STR = \"SYSTEM\"\n",
        "MSG_STR = \"message\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Here is how you can use the API to prompt the Cohere model. All the LLM APIs have pretty much the same format.\n",
        "# Docs: https://docs.cohere.com/reference/chat\n",
        "\n",
        "# We're providing one prompt format here, which we'll use as the \"baseline\" format.\n",
        "\n",
        "# Here's an example of one way you can provide a prompt and demonstrations to the model, through the chat history.\n",
        "# Here, we provide the initial prompt using the SYSTEM role, then provide each example (here, just one) as a USER, SYSTEM interaction.\n",
        "chat_history = [\n",
        "    {'role': SYSTEM_STR, MSG_STR:\n",
        "     \"\"\"You will be given input text containing different types of entities that you will label.\n",
        "     This is the list of entity types to label: Deity, Mythological_king, Cretaceous_dinosaur, Aquatic_mammal, Aquatic_animal, Goddess.\n",
        "     Label the enities by surrounding them with tags like '<Cretaceous_dinosaur> Beipiaognathus </Cretaceous_dinosaur>'.\"\"\"\n",
        "     }, \n",
        "     {'role': USER_STR, MSG_STR: \"\"\"Text: Once paired in later myths with her Titan brother Hyperion as her husband, mild-eyed Euryphaessa, the far-shining one of the Homeric Hymn to Helios, was said to be the mother of Helios (the Sun), Selene (the Moon), and Eos (the Dawn).\"\"\"},\n",
        "     {'role': SYSTEM_STR, MSG_STR: \"\"\"Labels: Once paired in later myths with her Titan brother <Deity> Hyperion </Deity> as her husband, mild-eyed Euryphaessa, the far-shining one of the Homeric Hymn to Helios, was said to be the mother of Helios (the Sun), <Goddess> Selene </Goddess> (the Moon), and <Goddess> Eos </Goddess> (the Dawn).\"\"\"\n",
        "}]\n",
        "\n",
        "# This is where you provide the final prompt that we want the model to complete to give us the answer.\n",
        "message = f\"\"\"Text: From her ideological conception, Taweret was closely grouped with (and is often indistinguishable from) several other protective hippopotamus goddesses: Ipet, Reret, and Hedjet.\n",
        "Labels: \"\"\"\n",
        "\n",
        "response = co.chat(\n",
        "    model=\"command-r-plus\",\n",
        "    temperature=0.0,\n",
        "    chat_history=chat_history,\n",
        "    message=message\n",
        ")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# OpenAI backend\n",
        "!pip install openai\n",
        "from openai import OpenAI\n",
        "\n",
        "# Use the API key that we \n",
        "client = OpenAI(api_key='OPENAI-KEY', base_url=\"https://cmu.litellm.ai\")\n",
        "USER_STR = \"user\"\n",
        "SYSTEM_STR = \"system\"\n",
        "MSG_STR = \"content\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Here is how you can use the API to prompt the OpenAI model using the same prompt as we used above for Cohere. \n",
        "# Docs: https://platform.openai.com/docs/api-reference\n",
        "messages = [\n",
        "    {'role': SYSTEM_STR, MSG_STR:\n",
        "     \"\"\"You will be given input text containing different types of entities that you will label.\n",
        "     This is the list of entity types to label: Deity, Mythological_king, Cretaceous_dinosaur, Aquatic_mammal, Aquatic_animal, Goddess.\n",
        "     Label the enities by surrounding them with tags like '<Cretaceous_dinosaur> Beipiaognathus </Cretaceous_dinosaur>'.\"\"\"\n",
        "     }, \n",
        "     {'role': USER_STR, MSG_STR: \"\"\"Text: Once paired in later myths with her Titan brother Hyperion as her husband, mild-eyed Euryphaessa, the far-shining one of the Homeric Hymn to Helios, was said to be the mother of Helios (the Sun), Selene (the Moon), and Eos (the Dawn).\"\"\"},\n",
        "     {'role': SYSTEM_STR, MSG_STR: \"\"\"Labels: Once paired in later myths with her Titan brother <Deity> Hyperion </Deity> as her husband, mild-eyed Euryphaessa, the far-shining one of the Homeric Hymn to Helios, was said to be the mother of Helios (the Sun), <Goddess> Selene </Goddess> (the Moon), and <Goddess> Eos </Goddess> (the Dawn).\"\"\"},\n",
        "     {'role': USER_STR, MSG_STR: \"\"\"Text: From her ideological conception, Taweret was closely grouped with (and is often indistinguishable from) several other protective hippopotamus goddesses: Ipet, Reret, and Hedjet.\\nLabels: \"\"\"}\n",
        "]\n",
        "print(messages)\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    temperature=0.0,\n",
        "    seed=random_seed,\n",
        "    messages=messages\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)\n",
        "\n",
        "# You can also print out the usage, in number of tokens. \n",
        "# Pricing is per input/output token, listed here: https://openai.com/pricing\n",
        "print(f\"Usage: {response.usage.prompt_tokens} input, {response.usage.completion_tokens} output, {response.usage.total_tokens} total tokens\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "from datasets import Dataset, ClassLabel, Sequence\n",
        "\n",
        "data_splits = load_dataset('json', data_files={'train': 'dinos_and_deities_train_bio.jsonl', 'dev': 'dinos_and_deities_dev_bio_sm.jsonl', 'test': 'dinos_and_deities_test_bio_nolabels.jsonl'})\n",
        "\n",
        "# Load dicts for mapping int labels to strings, and vice versa\n",
        "label_names_fname = \"dinos_and_deities_train_bio.jsonl.labels\"\n",
        "labels_int2str = []\n",
        "with open(label_names_fname) as f:\n",
        "    labels_int2str = f.read().split()\n",
        "print(f\"Labels: {labels_int2str}\")\n",
        "labels_str2int = {l: i for i, l in enumerate(labels_int2str)}\n",
        "\n",
        "# Also create a set containing the original labels, without B- and I- tags\n",
        "orig_labels = set()\n",
        "for label in labels_str2int.keys():\n",
        "    orig_label = label[2:]\n",
        "    if orig_label:\n",
        "        orig_labels.add(orig_label)\n",
        "print(f\"Orig labels: {orig_labels}\")\n",
        "\n",
        "# data_splits.cast_column(\"ner_tags\", Sequence(ClassLabel(names=labels_int2str)))\n",
        "print(data_splits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's inspect a single example\n",
        "dev_example = data_splits['dev'][5]\n",
        "\n",
        "print(json.dumps(dev_example, indent=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ok, now let's make the prompting a bit more programmatic. First, implement a function that takes an example from\n",
        "# the dataset, and converts it into a message for the model using the format we specified above. \n",
        "\n",
        "def get_message(example):\n",
        "    content = example['content']\n",
        "    message = f'Text: {content}\\nLabels: '\n",
        "    return message"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Next we're going to implement a function to return the chat_history, but in order to do that we first need\n",
        "# to be able to convert labeled examples from the dataset into a format that makes more sense for the model,\n",
        "# in this case the HTML-style format we specified in the example. That's the task for this function: take\n",
        "# an example from the dataset as input, and return a string that has tagged the text with labels in the given\n",
        "# HTML-style format.\n",
        "# \n",
        "def convert_bio_to_prompt(example):\n",
        "    prohibited_occurences = ['</Goddess> <Goddess>', '<Aquatic_mammal> </Aquatic_mammal>', '<Mythological_king> </Mythological_king>', '<Aquatic_animal> </Aquatic_animal>', '<Deity> </Deity>', '<Cretaceous_dinosaur> </Cretaceous_dinosaur>']\n",
        "    omit = 'O'\n",
        "    full_stop = ['.', ':', ',']\n",
        "    message = []\n",
        "    for i,j in zip(example['tokens'], example['ner_strings']):\n",
        "        if j is not omit:\n",
        "            if i[-1] in full_stop:\n",
        "                message.append(\"<\"+j[2:]+\">\")\n",
        "                message.append(i[:-1])\n",
        "                message.append(\"</\"+j[2:]+\">\"+ i[-1])\n",
        "            else:\n",
        "                message.append(\"<\"+j[2:]+\">\")\n",
        "                message.append(i)\n",
        "                message.append(\"</\"+j[2:]+\">\")\n",
        "        else:\n",
        "            message.append(i)\n",
        "    output = \" \".join(message)\n",
        "    big_regex = re.compile('|'.join(map(re.escape, prohibited_occurences)))\n",
        "    output = big_regex.sub(\"\", output)\n",
        "    output = output.replace(\"  \", \" \")\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "label_example = convert_bio_to_prompt(dev_example)\n",
        "print(label_example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# sample selection, balance the number of examples for each label\n",
        "# sample 10 examples for each label\n",
        "dataset = data_splits['train']\n",
        "print(dataset['ner_strings'][0])\n",
        "plain_tags = [[i[2:] if i not in ['O'] else i for i in ner_strings] for ner_strings in dataset['ner_strings']]\n",
        "print(plain_tags)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# rank the tag_type_count in decreasing order of diversity but also keep the original index\n",
        "from collections import Counter\n",
        "tag_type_count = [(i, tag_count) for i, tag_count in enumerate(tag_type_count)]\n",
        "tag_type_count.sort(key=lambda x: len(x[1]), reverse=True)\n",
        "print(tag_type_count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sorted_indices = [i for i, _ in tag_type_count]\n",
        "print(sorted_indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 200,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Now we can write a function that takes the number of shots, dataset, list of entity types, and \n",
        "# convert_bio_to_prompt function, and returns the chat_history (a list of maps) structured as in \n",
        "# the example.\n",
        "#\n",
        "def get_chat_history(shots, dataset, entity_types_list, convert_bio_to_prompt_fn):\n",
        "    samples = random.sample(range(len(dataset)), shots)\n",
        "    message = [{'role': SYSTEM_STR, MSG_STR:\n",
        "     \"\"\"You will be given input text containing different types of entities that you will label.\n",
        "     This is the list of entity types to label: Deity, Mythological_king, Cretaceous_dinosaur, Aquatic_mammal, Aquatic_animal, Goddess. \n",
        "     Label the enities by surrounding them with tags like '<Cretaceous_dinosaur> Beipiaognathus </Cretaceous_dinosaur>'.\n",
        "     \"\"\"\n",
        "     }]\n",
        "    for i in samples:\n",
        "        user   = dataset[i]['content']\n",
        "        system = convert_bio_to_prompt_fn(dataset[i])\n",
        "        message.append({'role': f'{USER_STR}',  f'{MSG_STR}': f\"\"\"Text: {user}\"\"\"})\n",
        "        message.append({'role': f'{SYSTEM_STR}', f'{MSG_STR}': f\"\"\"Labels: {system}\"\"\"})\n",
        "    return message"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Now we can put all of those together to prompt the model more automagically!\n",
        "\n",
        "# For Cohere:\n",
        "num_shots = 20\n",
        "response = co.chat(\n",
        "    model=\"command-r-plus\",\n",
        "    temperature=0.0,\n",
        "    chat_history=get_chat_history(num_shots, data_splits['train'], orig_labels, convert_bio_to_prompt),\n",
        "    message=get_message(dev_example)\n",
        ")\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# For OpenAI:\n",
        "num_shots = 0\n",
        "\n",
        "chat_history = get_chat_history(num_shots, data_splits['train'], orig_labels, convert_bio_to_prompt)\n",
        "message = {'role': USER_STR, MSG_STR: get_message(dev_example)}\n",
        "chat_history.append(message)\n",
        "print(chat_history)\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    temperature=0.0,\n",
        "    seed=random_seed,\n",
        "    messages=chat_history\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)\n",
        "print(response.choices[0].message.content[:7])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 204,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Now let's wrap that call in a function that takes shots and an example, calls the API and returns the response.\n",
        "\n",
        "# Cohere:\n",
        "def call_api_cohere(shots, example):\n",
        "    success = False\n",
        "    while not success:\n",
        "        try:\n",
        "            response = co.chat(\n",
        "                model=\"command-r-plus\",\n",
        "                temperature=0.0,\n",
        "                chat_history=get_chat_history(shots, data_splits['train'], orig_labels, convert_bio_to_prompt),\n",
        "                message=get_message(example)\n",
        "            )    \n",
        "            success = 1\n",
        "        except Exception as err:\n",
        "            tqdm.write(f\"Caught exception: {err}\")\n",
        "    return response.text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 214,
      "metadata": {},
      "outputs": [],
      "source": [
        "# OpenAI:\n",
        "def call_api_openai(shots, example):\n",
        "    success = False\n",
        "    while not success:\n",
        "        try:\n",
        "            chat_history = get_chat_history(shots, data_splits['train'], orig_labels, convert_bio_to_prompt)\n",
        "            message = {'role': USER_STR, 'content': get_message(example)}\n",
        "            chat_history.append(message)\n",
        "            response = client.chat.completions.create(\n",
        "                model=\"gpt-3.5-turbo\",\n",
        "                temperature=0.0,\n",
        "                messages=chat_history\n",
        "            )\n",
        "            success = 1\n",
        "        except Exception as err:\n",
        "            tqdm.write(f\"Caught exception: {err}\")\n",
        "    return response.choices[0].message.content "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 215,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Now we want to be able to evaluate the model, in order to compare it to e.g. the fine-tuned BERT model.\n",
        "# In order to do this, we need to write the reverse of the convert_bio_to_prompt function, so that we can\n",
        "# convert in the other direction, from the generated response in prompt format, back to bio for evaluation\n",
        "# using seqeval.\n",
        "\n",
        "# The input to this function is the string response from the model, and the output should be a list of \n",
        "# text BIO labels corresponding to the labeling implied by the tagged output produced by the model, as \n",
        "# well as the list of tokens (since the generative model could return something different than we gave it,\n",
        "# and we need to handle that somehow in the eval).\n",
        "\n",
        "def convert_response_to_bio(response):\n",
        "    text = []\n",
        "    labels = []\n",
        "    current_label = None\n",
        "    \n",
        "    if response[:7] == 'Labels:':\n",
        "        response = response[7:]\n",
        "    elif response[:5] == 'Text:':\n",
        "        response = response[5:]\n",
        "    \n",
        "    # Split the response into tokens\n",
        "    tokens = re.split(r'(\\s+|<[A-Za-z_]+>|</[A-Za-z_]+>)', response)\n",
        "    tokens = [token for token in tokens if token.strip()]\n",
        "\n",
        "\n",
        "    if not tokens:\n",
        "        return [], []\n",
        "\n",
        "    for i in range(len(tokens) - 1):\n",
        "        if text != [] and tokens[i] in string.punctuation:\n",
        "            text[-1] += tokens[i]\n",
        "        elif tokens[i].startswith('<'):\n",
        "            if tokens[i].startswith(\"</\"):\n",
        "                current_label = None\n",
        "            else:\n",
        "                current_label = 'B-' + tokens[i][1:-1]\n",
        "        elif i > 0 and (tokens[i - 1].startswith('<') or tokens[i + 1].startswith(\"</\")):\n",
        "            if current_label:\n",
        "                labels.append(current_label)\n",
        "                text.append(tokens[i])\n",
        "                current_label = None\n",
        "            elif labels != [] and labels[-1] != 'O' and not tokens[i-1].startswith('</'):\n",
        "                labels.append('I-' + labels[-1].split('-')[1])\n",
        "                text.append(tokens[i])\n",
        "            else:\n",
        "                labels.append('O')\n",
        "                text.append(tokens[i])\n",
        "        elif labels != [] and labels[-1] != 'O' and not tokens[i-1].startswith('</') and tokens[i-1] not in string.punctuation: # previous label strt\n",
        "            labels.append('I-' + labels[-1].split('-')[1])\n",
        "            text.append(tokens[i])\n",
        "        else:\n",
        "            labels.append('O')\n",
        "            text.append(tokens[i])\n",
        "\n",
        "    if tokens[-1] in string.punctuation:\n",
        "        text[-1] += tokens[-1]\n",
        "    elif not tokens[-1].startswith('<'):\n",
        "        labels.append('O')\n",
        "        text.append(tokens[-1])\n",
        "    return labels, text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Here's a test example you can use to validate/debug your code (note that this was constructed to simulate various\n",
        "# spacing/tokenization scenarios and does not necessarily reflect \"correct\" labeling wrt the training data):\n",
        "import ipytest\n",
        "ipytest.autoconfig()\n",
        "def test_convert_html_to_bio():\n",
        "    html_str = 'From <Goddess> her</Goddess> ideological conception, <Goddess> the deity Taweret </Goddess> was closely grouped with (and is often indistinguishable from) several other protective <Aquatic_mammal>hippopotamus</Aquatic_mammal> <Goddess>goddesses </Goddess>: <Goddess> Ipet (\"the Nurse\")</Goddess>, <Goddess>Reret (\"the Sow\") </Goddess>, and <Goddess>Hedjet (\"the White One\")</Goddess>.'\n",
        "    labels, text = convert_response_to_bio(html_str)\n",
        "    true_labels = ['O', 'B-Goddess', 'O', 'O', 'B-Goddess', 'I-Goddess', 'I-Goddess', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Aquatic_mammal', 'B-Goddess', 'B-Goddess', 'I-Goddess', 'I-Goddess', 'B-Goddess', 'I-Goddess', 'I-Goddess', 'O', 'B-Goddess', 'I-Goddess', 'I-Goddess', 'I-Goddess']\n",
        "    true_text = ['From', 'her', 'ideological', 'conception,', 'the', 'deity', 'Taweret', 'was', 'closely', 'grouped', 'with', '(and', 'is', 'often', 'indistinguishable', 'from)', 'several', 'other', 'protective', 'hippopotamus', 'goddesses:', 'Ipet', '(\"the', 'Nurse\"),', 'Reret', '(\"the', 'Sow\"),', 'and', 'Hedjet', '(\"the', 'White', 'One\").']\n",
        "    print(labels)\n",
        "    print(text)\n",
        "    assert labels == true_labels\n",
        "    assert text == true_text\n",
        "\n",
        "def test_convert_html_to_bio_labels():\n",
        "    html_str = 'Labels: From <Goddess> her</Goddess> ideological conception, <Goddess> the deity Taweret </Goddess> was closely grouped with (and is often indistinguishable from) several other protective <Aquatic_mammal>hippopotamus</Aquatic_mammal> <Goddess>goddesses </Goddess>: <Goddess> Ipet (\"the Nurse\")</Goddess>, <Goddess>Reret (\"the Sow\") </Goddess>, and <Goddess>Hedjet (\"the White One\")</Goddess>.'\n",
        "    labels, text = convert_response_to_bio(html_str)\n",
        "    true_labels = ['O', 'B-Goddess', 'O', 'O', 'B-Goddess', 'I-Goddess', 'I-Goddess', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Aquatic_mammal', 'B-Goddess', 'B-Goddess', 'I-Goddess', 'I-Goddess', 'B-Goddess', 'I-Goddess', 'I-Goddess', 'O', 'B-Goddess', 'I-Goddess', 'I-Goddess', 'I-Goddess']\n",
        "    true_text = ['From', 'her', 'ideological', 'conception,', 'the', 'deity', 'Taweret', 'was', 'closely', 'grouped', 'with', '(and', 'is', 'often', 'indistinguishable', 'from)', 'several', 'other', 'protective', 'hippopotamus', 'goddesses:', 'Ipet', '(\"the', 'Nurse\"),', 'Reret', '(\"the', 'Sow\"),', 'and', 'Hedjet', '(\"the', 'White', 'One\").']\n",
        "    print(labels)\n",
        "    print(text)\n",
        "    assert labels == true_labels\n",
        "    assert text == true_text\n",
        "\n",
        "ipytest.run('-vv')  # '-vv' for increased verbosity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 217,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Now we can put all of the above together to evaluate!\n",
        "metric = evaluate.load(\"seqeval\")\n",
        "\n",
        "def run_eval(dataset, shots, backend):\n",
        "\n",
        "  for example in tqdm(dataset, total=len(dataset), desc=\"Evaluating\", position=tqdm._get_free_pos()):\n",
        "\n",
        "      # String list of labels (BIO)\n",
        "      true_labels = [labels_int2str[l] for l in example['ner_tags']]\n",
        "      example_tokens = example['tokens']\n",
        "\n",
        "      response_text = call_api_openai(shots, example) if backend == \"openai\" else call_api_cohere(shots, example)\n",
        "      # print(f\"Response: {response_text}\")\n",
        "\n",
        "      # String list of predicted labels (BIO)\n",
        "      predictions, generated_tokens = convert_response_to_bio(response_text)\n",
        "\n",
        "      # Handle case where the generated text doesn't align with the input text.\n",
        "      # Basically, we'll eval everything up to where the two strings start to diverge.\n",
        "      # We relax this slightly by ignoring punctuation (sometimes we lose a paren or something, \n",
        "      # but that's not catastrophic for eval/tokenization).\n",
        "      # Just predict 'O' for anything following mismatch.\n",
        "      matching_elements = [strip_punct(i) == strip_punct(j) for i, j in zip(example_tokens, generated_tokens)]\n",
        "\n",
        "      if False in matching_elements:\n",
        "         last_matching_idx = matching_elements.index(False)\n",
        "      else:\n",
        "         last_matching_idx = min(len(generated_tokens), len(example_tokens))\n",
        "\n",
        "      predictions = predictions[:last_matching_idx] + ['O']*(len(example_tokens)-last_matching_idx)\n",
        "      metric.add(predictions=predictions, references=true_labels)\n",
        "  \n",
        "  return metric.compute()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run the eval on the dev set\n",
        "dev_examples_to_take = 0\n",
        "\n",
        "dev_set = data_splits['dev']\n",
        "if dev_examples_to_take > 0:\n",
        "    dev_set = data_splits['dev'].select(range(dev_examples_to_take))\n",
        "\n",
        "for num_shots in [0, 1, 5, 10, 20, 40]:\n",
        "    result = run_eval(dev_set, shots=num_shots, backend='openai')\n",
        "    print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Output for Evaluation\n",
        "\n",
        "In the following cells, run your trained model on the test data, and produce a list of lists of tags, with one list per sentence, e.g. \n",
        "\n",
        "```\n",
        "[\n",
        "    [\n",
        "        \"B-Aquatic_animal\",\n",
        "        \"I-Aquatic_animal\",\n",
        "        \"I-Aquatic_animal\",\n",
        "...\n",
        "        \"O\",\n",
        "        \"O\",\n",
        "        \"B-Aquatic_animal\",\n",
        "        \"I-Aquatic_animal\"\n",
        "    ],\n",
        "    [...]\n",
        "]\n",
        "```\n",
        "\n",
        "Serialize your predictions into a file named `test_predictions_llm_baseline.json` for your initial attempt at an LLM tagger."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# your code here.\n",
        "test_prediction = []\n",
        "for i, example in enumerate(tqdm(data_splits['test'], total = len(data_splits['test']), desc=\"Testing\", position=tqdm._get_free_pos())):\n",
        "    # call the llm api\n",
        "    response_text = call_api_openai(5, example)\n",
        "\n",
        "    # convert the response to BIO format\n",
        "    predictions, generated_tokens = convert_response_to_bio(response_text)\n",
        "\n",
        "    # Handle case where the generated text doesn't align with the input text.\n",
        "    if len(predictions) > len(example['tokens']):\n",
        "        test_prediction.append(predictions[:len(example['tokens'])])\n",
        "    else:\n",
        "        mismatch = len(example['tokens']) - len(predictions)\n",
        "        test_prediction.append(predictions + ['O']*mismatch)\n",
        "\n",
        "# Save the predictions to a file\n",
        "with open(\"test_predictions_llm_baseline.json\", \"w\") as f:\n",
        "    json.dump(test_prediction, f)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "vlms",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
